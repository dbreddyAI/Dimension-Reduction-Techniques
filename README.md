# Dimension-Reduction-Techniques

# [Principle component Analysis](https://github.com/piyushpathak03/Dimension-Reduction-Techniques/blob/master/PCA.ipynb)
![alt text](https://github.com/piyushpathak03/Dimension-Reduction-Techniques/blob/master/pca.gif)
<br />

Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.

Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.

So to sum up, the idea of PCA is simple â€” reduce the number of variables of a data set, while preserving as much information as possible.

![alt text](https://github.com/piyushpathak03/Dimension-Reduction-Techniques/blob/master/PCA1.gif)
<br />

# [Linear discriminant analysis](https://github.com/piyushpathak03/Dimension-Reduction-Techniques/blob/master/LDA.ipynb)
LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements.[1][2] However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label).[3] Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.

LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data.[4] LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.
![alt text](https://github.com/piyushpathak03/Dimension-Reduction-Techniques/blob/master/LDA.gif)
<br />

# [Kernel Principle component Analysis](https://github.com/piyushpathak03/Dimension-Reduction-Techniques/blob/master/KPCA.ipynb)
![alt text](https://github.com/piyushpathak03/Dimension-Reduction-Techniques/blob/master/KPCA.gif)
<br />
KPCA is a nonlinear PCA developed by using the kernel method.Kernel functions are used similar to SVM and mostly similar to PCA
![alt text](https://github.com/piyushpathak03/Dimension-Reduction-Techniques/blob/master/KPCA%201.gif)
<br />

## About me

**Piyush Pathak**

[**PORTFOLIO**](https://anirudhrapathak3.wixsite.com/piyush)

[**GITHUB**](https://github.com/piyushpathak03)

[**BLOG**](https://medium.com/@piyushpathak03)


# ðŸ“« Follw me: 

[![Linkedin Badge](https://img.shields.io/badge/-PiyushPathak-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/piyushpathak03/)](https://www.linkedin.com/in/piyushpathak03/)

<p  align="right"><img height="100" src = "https://media.giphy.com/media/l3URDstnIjBNY7rwLB/giphy.gif"></p>
